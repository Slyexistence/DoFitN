import os
import random
import numpy as np
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding
from sklearn.metrics import accuracy_score, f1_score
from collections import Counter


os.environ["HF_DATASETS_CACHE"] = r"D:\cash"

# –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏ (–∫–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏, —Ä–∞–∑–¥–µ–ª—ë–Ω–Ω—ã–µ —Ç–∞–±—É–ª—è—Ü–∏–µ–π: topic –∏ message)
DATA_FILE = r"D:\–ü—Ä–æ–µ–∫—Ç—ã\–î–∏–ø–ª–æ–º–∞–Ω—è —Ä–∞–±–æ—Ç–∞\DoFitN\Code\DoFitN\new_code\data_csv\combined_features.csv"
SAVE_PATH = r"D:\–ü—Ä–æ–µ–∫—Ç—ã\–î–∏–ø–ª–æ–º–∞–Ω—è —Ä–∞–±–æ—Ç–∞\DoFitN\Code\DoFitN\Pipline\save"

# –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –∫–ª—é—á–∞ "train"
dataset = load_dataset("csv", data_files={"train": DATA_FILE}, delimiter="\t", column_names=["topic", "message"])

# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ç–µ–º (—Ç–∏–ø–æ–≤ –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞)
topics = list(set(dataset["train"]["topic"]))
topics.sort()  # –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏
print("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Ç–∏–ø—ã –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞:", topics)

# –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–µ–º—ã –≤ —á–∏—Å–ª–æ–≤–æ–π –∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä
topic2id = {topic: idx for idx, topic in enumerate(topics)}
id2topic = {idx: topic for topic, idx in topic2id.items()}

def map_labels(example):
    example["label"] = topic2id[example["topic"]]
    return example

dataset = dataset["train"].map(map_labels)

# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –∑–∞–¥–∞–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ–º –≤ –ø–æ–ª–µ "text"
def add_prompt(example):
    prompt = ("–û–ø—Ä–µ–¥–µ–ª–∏—Ç–µ, –∫ –∫–∞–∫–æ–º—É —Ç–∏–ø—É –º–æ—à–µ–Ω–Ω–∏—á–µ—Å—Ç–≤–∞ –æ—Ç–Ω–æ—Å–∏—Ç—Å—è —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ. "
              "–í–æ–∑–º–æ–∂–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã: " + ", ".join(topics) + ". –°–æ–æ–±—â–µ–Ω–∏–µ:")
    example["text"] = f"{prompt} {example['message']}"
    return example

dataset = dataset.map(add_prompt)

# –ü–µ—Ä–µ–º–µ—à–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è —Å–ª—É—á–∞–π–Ω–æ–≥–æ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
dataset = dataset.shuffle(seed=42)
print("–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–µ—Ç–æ–∫:", Counter(dataset["label"]))

# –†–∞–∑–±–∏–≤–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ —Ç–µ—Å—Ç–æ–≤—É—é –≤—ã–±–æ—Ä–∫–∏ (80/20)
split_dataset = dataset.train_test_split(test_size=0.2, seed=42)
data_dict = DatasetDict({
    "train": split_dataset["train"],
    "test": split_dataset["test"]
})

# –í—ã–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞
model_checkpoint = "DeepPavlov/rubert-base-cased"
tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)
num_labels = len(topics)
model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)

# –§—É–Ω–∫—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ü–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–æ–ª–µ "text")
def tokenize_function(example):
    return tokenizer(example["text"], truncation=True, padding="max_length", max_length=128)

tokenized_datasets = data_dict.map(tokenize_function, batched=True)
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

# –§—É–Ω–∫—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –º–µ—Ç—Ä–∏–∫: accuracy –∏ –≤–∑–≤–µ—à–µ–Ω–Ω—ã–π F1
def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, predictions)
    f1 = f1_score(labels, predictions, average="weighted")
    return {"accuracy": acc, "f1": f1}

# –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
training_args = TrainingArguments(
    output_dir="./finetune_fraud_results",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    learning_rate=1.5e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=5,
    weight_decay=0.01,
    logging_steps=50,
    load_best_model_at_end=True,
)

# –°–æ–∑–¥–∞–µ–º Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["test"],
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
)

# –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è (Trainer –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç GPU, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω)
trainer.train()

# –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
results = trainer.evaluate()
print("Evaluation results:", results)

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–æ–æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞
model.save_pretrained(SAVE_PATH)
tokenizer.save_pretrained(SAVE_PATH)
print(f"Model and tokenizer saved in {SAVE_PATH}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
predictions_output = trainer.predict(tokenized_datasets["test"])
pred_labels = np.argmax(predictions_output.predictions, axis=1)
true_labels = predictions_output.label_ids

# –í—ã–≤–æ–¥–∏–º –º–∞—Ç—Ä–∏—Ü—É –æ—à–∏–±–æ–∫
cm = confusion_matrix(true_labels, pred_labels)
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=topics)
fig, ax = plt.subplots(figsize=(10, 10))
disp.plot(ax=ax, xticks_rotation=45, cmap="Blues", colorbar=False)
plt.title("–ú–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ (Confusion Matrix)")
plt.tight_layout()
plt.show()

# –í—ã–≤–æ–¥–∏–º —Ç–æ–ø –æ—à–∏–±–æ–∫
from collections import defaultdict
errors = defaultdict(list)
for i, (true, pred) in enumerate(zip(true_labels, pred_labels)):
    if true != pred:
        true_name = id2topic[true]
        pred_name = id2topic[pred]
        msg = data_dict["test"][i]["message"]
        errors[(true_name, pred_name)].append(msg)

# –ü–æ–∫–∞–∑—ã–≤–∞–µ–º 3 –ø—Ä–∏–º–µ—Ä–∞ —Å–∞–º—ã—Ö —á–∞—Å—Ç—ã—Ö –æ—à–∏–±–æ–∫
print("\nüîç –¢–æ–ø –æ—à–∏–±–æ–∫ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏:\n")
sorted_errors = sorted(errors.items(), key=lambda x: len(x[1]), reverse=True)
for (true_name, pred_name), msgs in sorted_errors[:3]:
    print(f"‚ùå –ò—Å—Ç–∏–Ω–Ω–∞—è –º–µ—Ç–∫–∞: {true_name} ‚Üí –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–æ –∫–∞–∫: {pred_name} ({len(msgs)} —Ä–∞–∑)")
    for m in msgs[:3]:  # –ü–æ–∫–∞–∂–µ–º —Ç–æ–ª—å–∫–æ 3 –ø—Ä–∏–º–µ—Ä–∞, —á—Ç–æ–±—ã –Ω–µ –∑–∞—Ö–ª–∞–º–ª—è—Ç—å
        print("   ‚û§", m)
    print()
