{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '4c:5f:70:98:4d:bf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 443\u001b[0m\n\u001b[0;32m    440\u001b[0m      \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mМодель создана и готова к обучению.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    442\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 443\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 422\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# Масштабирование признаков\u001b[39;00m\n\u001b[0;32m    421\u001b[0m  scaler \u001b[38;5;241m=\u001b[39m StandardScaler()\n\u001b[1;32m--> 422\u001b[0m  X_scaled \u001b[38;5;241m=\u001b[39m \u001b[43mscaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;66;03m# Разделение на обучающую и тестовую выборки\u001b[39;00m\n\u001b[0;32m    425\u001b[0m  X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(X_scaled, y, test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Slyexistence\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 313\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    314\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    315\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    316\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    317\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    318\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    319\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Slyexistence\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1084\u001b[0m             (\n\u001b[0;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[0;32m   1094\u001b[0m         )\n\u001b[0;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[0;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Slyexistence\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:878\u001b[0m, in \u001b[0;36mStandardScaler.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# Reset internal state before fitting\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()\n\u001b[1;32m--> 878\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Slyexistence\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Slyexistence\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:914\u001b[0m, in \u001b[0;36mStandardScaler.partial_fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    882\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Online computation of mean and std on X for later scaling.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \n\u001b[0;32m    884\u001b[0m \u001b[38;5;124;03mAll of X is processed as a single batch. This is intended for cases\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    911\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    913\u001b[0m first_call \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_samples_seen_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 914\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfirst_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m n_features \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m    923\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Slyexistence\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\Slyexistence\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\validation.py:1012\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1010\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1011\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1012\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1015\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1016\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Slyexistence\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:751\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp, device)\u001b[0m\n\u001b[0;32m    749\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 751\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    754\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: '4c:5f:70:98:4d:bf'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Класс для извлечения признаков из ARP-пакетов\n",
    "class ARPFeatureExtractor:\n",
    "    def __init__(self, window_size=10, history_window=60):\n",
    "        self.window_size = window_size  # окно в секундах для текущих метрик\n",
    "        self.history_window = history_window  # окно для исторических данных\n",
    "        self.mac_ip_mapping = {}  # словарь для отслеживания MAC-IP связей\n",
    "        self.ip_mac_mapping = {}  # словарь для отслеживания IP-MAC связей\n",
    "        self.packet_history = []  # история пакетов\n",
    "        self.gateway_mac = None  # MAC-адрес шлюза (определяется автоматически)\n",
    "        \n",
    "    def extract_features(self, packets):\n",
    "        \"\"\"Извлечение признаков из набора пакетов\"\"\"\n",
    "        self.packet_history.extend(packets)\n",
    "        # Оставляем только пакеты в пределах истории\n",
    "        current_time = max([p['timestamp'] for p in packets])\n",
    "        self.packet_history = [p for p in self.packet_history \n",
    "                              if current_time - p['timestamp'] <= self.history_window]\n",
    "        \n",
    "        # Определяем шлюз, если еще не определен\n",
    "        if not self.gateway_mac:\n",
    "            self._identify_gateway()\n",
    "        \n",
    "        # Вычисляем базовые признаки\n",
    "        features = {}\n",
    "        window_packets = [p for p in self.packet_history \n",
    "                         if current_time - p['timestamp'] <= self.window_size]\n",
    "        \n",
    "        # 1. Дубликаты\n",
    "        features['duplicate_count'] = self._count_duplicates(window_packets)\n",
    "        features['duplicate_ratio'] = features['duplicate_count'] / len(window_packets) if window_packets else 0\n",
    "        \n",
    "        # 2. Два IP имеют один MAC\n",
    "        self._update_mappings(window_packets)\n",
    "        features['ips_per_mac'] = self._calculate_ips_per_mac()\n",
    "        features['ip_mac_entropy'] = self._calculate_ip_mac_entropy()\n",
    "        \n",
    "        # 3. Код операции\n",
    "        req_count = sum(1 for p in window_packets if p['opcode'] == 1)\n",
    "        rep_count = sum(1 for p in window_packets if p['opcode'] == 2)\n",
    "        features['request_reply_ratio'] = req_count / rep_count if rep_count else float('inf')\n",
    "        \n",
    "        # 4. Отсутствие запроса, а чисто ответ\n",
    "        features['unsolicited_replies'] = self._count_unsolicited_replies(window_packets)\n",
    "        features['unsolicited_reply_ratio'] = features['unsolicited_replies'] / rep_count if rep_count else 0\n",
    "        \n",
    "        # 5. Высокий объём трафика\n",
    "        features['packets_per_second'] = len(window_packets) / self.window_size\n",
    "        features['traffic_zscore'] = self._calculate_traffic_zscore(features['packets_per_second'])\n",
    "        \n",
    "        # 6. Частые широковещательные письма не от шлюза\n",
    "        features['non_gateway_broadcasts'] = self._count_non_gateway_broadcasts(window_packets)\n",
    "        features['broadcast_entropy'] = self._calculate_broadcast_entropy(window_packets)\n",
    "        \n",
    "        # ---- ГРАФОВЫЕ МЕТРИКИ ----\n",
    "        graph_features = self._calculate_graph_metrics(window_packets)\n",
    "        features.update(graph_features)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def _identify_gateway(self):\n",
    "        \"\"\"Определение MAC-адреса шлюза на основе активности\"\"\"\n",
    "        # В простейшем случае - MAC с наибольшим количеством ARP ответов\n",
    "        mac_reply_count = {}\n",
    "        for p in self.packet_history:\n",
    "            if p['opcode'] == 2:  # ARP ответ\n",
    "                mac = p['src_mac']\n",
    "                mac_reply_count[mac] = mac_reply_count.get(mac, 0) + 1\n",
    "        \n",
    "        if mac_reply_count:\n",
    "            self.gateway_mac = max(mac_reply_count, key=mac_reply_count.get)\n",
    "    \n",
    "    def _count_duplicates(self, packets):\n",
    "        \"\"\"Подсчет дубликатов пакетов\"\"\"\n",
    "        packet_signatures = []\n",
    "        duplicates = 0\n",
    "        \n",
    "        for p in packets:\n",
    "            # Создаем подпись пакета (ключевые поля)\n",
    "            sig = (p['src_mac'], p['dst_mac'], p['src_ip'], p['dst_ip'], p['opcode'])\n",
    "            if sig in packet_signatures:\n",
    "                duplicates += 1\n",
    "            else:\n",
    "                packet_signatures.append(sig)\n",
    "        \n",
    "        return duplicates\n",
    "    \n",
    "    def _update_mappings(self, packets):\n",
    "        \"\"\"Обновление отображений MAC-IP и IP-MAC\"\"\"\n",
    "        for p in packets:\n",
    "            src_mac, src_ip = p['src_mac'], p['src_ip']\n",
    "            \n",
    "            # Обновляем MAC -> IP\n",
    "            if src_mac not in self.mac_ip_mapping:\n",
    "                self.mac_ip_mapping[src_mac] = set()\n",
    "            self.mac_ip_mapping[src_mac].add(src_ip)\n",
    "            \n",
    "            # Обновляем IP -> MAC\n",
    "            if src_ip not in self.ip_mac_mapping:\n",
    "                self.ip_mac_mapping[src_ip] = set()\n",
    "            self.ip_mac_mapping[src_ip].add(src_mac)\n",
    "    \n",
    "    def _calculate_ips_per_mac(self):\n",
    "        \"\"\"Среднее количество IP на один MAC\"\"\"\n",
    "        if not self.mac_ip_mapping:\n",
    "            return 0\n",
    "        return sum(len(ips) for ips in self.mac_ip_mapping.values()) / len(self.mac_ip_mapping)\n",
    "    \n",
    "    def _calculate_ip_mac_entropy(self):\n",
    "        \"\"\"Энтропия распределения IP для MAC\"\"\"\n",
    "        if not self.mac_ip_mapping:\n",
    "            return 0\n",
    "        \n",
    "        # Распределение количества IP на MAC\n",
    "        ip_counts = [len(ips) for ips in self.mac_ip_mapping.values()]\n",
    "        total = sum(ip_counts)\n",
    "        if total == 0:\n",
    "            return 0\n",
    "        \n",
    "        # Вычисляем энтропию\n",
    "        entropy = 0\n",
    "        for count in ip_counts:\n",
    "            prob = count / total\n",
    "            if prob > 0:\n",
    "                entropy -= prob * np.log2(prob)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    def _count_unsolicited_replies(self, packets):\n",
    "        \"\"\"Подсчет ответов без соответствующих запросов\"\"\"\n",
    "        # Создаем словарь запросов (target_ip -> source_ip)\n",
    "        requests = {}\n",
    "        unsolicited = 0\n",
    "        \n",
    "        # Сначала собираем все запросы\n",
    "        for p in packets:\n",
    "            if p['opcode'] == 1:  # ARP запрос\n",
    "                key = p['dst_ip']\n",
    "                if key not in requests:\n",
    "                    requests[key] = []\n",
    "                requests[key].append(p['src_ip'])\n",
    "        \n",
    "        # Теперь проверяем ответы\n",
    "        for p in packets:\n",
    "            if p['opcode'] == 2:  # ARP ответ\n",
    "                src_ip = p['src_ip']\n",
    "                if src_ip not in requests or not requests[src_ip]:\n",
    "                    unsolicited += 1\n",
    "        \n",
    "        return unsolicited\n",
    "    \n",
    "    def _calculate_traffic_zscore(self, current_rate):\n",
    "        \"\"\"Вычисление Z-score текущей скорости трафика\"\"\"\n",
    "        # Получаем исторические данные о скорости\n",
    "        if len(self.packet_history) < 2:\n",
    "            return 0\n",
    "        \n",
    "        # Разбиваем историю на окна\n",
    "        window_packets = {}\n",
    "        latest_time = max([p['timestamp'] for p in self.packet_history])\n",
    "        \n",
    "        # Группируем пакеты по окнам\n",
    "        for p in self.packet_history:\n",
    "            window_idx = int((latest_time - p['timestamp']) / self.window_size)\n",
    "            if window_idx not in window_packets:\n",
    "                window_packets[window_idx] = 0\n",
    "            window_packets[window_idx] += 1\n",
    "        \n",
    "        # Вычисляем скорости для окон\n",
    "        rates = [count / self.window_size for count in window_packets.values()]\n",
    "        \n",
    "        # Вычисляем среднее и стандартное отклонение\n",
    "        if not rates:\n",
    "            return 0\n",
    "        mean_rate = sum(rates) / len(rates)\n",
    "        std_rate = np.std(rates) if len(rates) > 1 else 1\n",
    "        \n",
    "        # Защита от деления на ноль\n",
    "        if std_rate == 0:\n",
    "            std_rate = 1\n",
    "        \n",
    "        # Возвращаем Z-score\n",
    "        return (current_rate - mean_rate) / std_rate\n",
    "    \n",
    "    def _count_non_gateway_broadcasts(self, packets):\n",
    "        \"\"\"Подсчет широковещательных пакетов не от шлюза\"\"\"\n",
    "        count = 0\n",
    "        for p in packets:\n",
    "            if p['is_broadcast'] and p['src_mac'] != self.gateway_mac:\n",
    "                count += 1\n",
    "        return count\n",
    "    \n",
    "    def _calculate_broadcast_entropy(self, packets):\n",
    "        \"\"\"Энтропия интервалов между широковещательными пакетами\"\"\"\n",
    "        broadcasts = [p for p in packets if p['is_broadcast']]\n",
    "        if len(broadcasts) < 2:\n",
    "            return 0\n",
    "        \n",
    "        # Сортируем по времени\n",
    "        broadcasts.sort(key=lambda p: p['timestamp'])\n",
    "        \n",
    "        # Вычисляем интервалы\n",
    "        intervals = [broadcasts[i+1]['timestamp'] - broadcasts[i]['timestamp'] \n",
    "                     for i in range(len(broadcasts)-1)]\n",
    "        \n",
    "        # Делаем дискретные bin-ы для интервалов\n",
    "        max_interval = max(intervals)\n",
    "        if max_interval == 0:\n",
    "            return 0\n",
    "            \n",
    "        bins = 10  # количество корзин\n",
    "        hist, _ = np.histogram(intervals, bins=bins, range=(0, max_interval), density=True)\n",
    "        \n",
    "        # Вычисляем энтропию\n",
    "        entropy = 0\n",
    "        for prob in hist:\n",
    "            if prob > 0:\n",
    "                entropy -= prob * np.log2(prob)\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    # ---- ГРАФОВЫЕ МЕТРИКИ ----\n",
    "    def _calculate_graph_metrics(self, packets):\n",
    "        \"\"\"Вычисление графовых метрик для анализа топологии сети\"\"\"\n",
    "        G = nx.Graph()\n",
    "        \n",
    "        # Создаем граф из ARP-пакетов\n",
    "        for p in packets:\n",
    "            src = p['src_mac']\n",
    "            dst = p['dst_mac']\n",
    "            G.add_node(src, type='mac')\n",
    "            G.add_node(dst, type='mac')\n",
    "            G.add_edge(src, dst, timestamp=p['timestamp'], opcode=p['opcode'])\n",
    "        \n",
    "        if len(G) <= 1:\n",
    "            return {\n",
    "                'graph_density': 0,\n",
    "                'avg_clustering': 0,\n",
    "                'avg_degree': 0,\n",
    "                'degree_centrality_entropy': 0,\n",
    "                'gateway_betweenness': 0,\n",
    "                'community_modularity': 0\n",
    "            }\n",
    "        \n",
    "        metrics = {}\n",
    "        \n",
    "        # 1. Плотность графа - показывает насколько связаны устройства\n",
    "        # Высокая плотность при атаке может указывать на сканирование сети\n",
    "        metrics['graph_density'] = nx.density(G)\n",
    "        \n",
    "        # 2. Средний кластерный коэффициент - насколько связаны соседи узлов\n",
    "        # При атаке часто снижается, т.к. атакующий нарушает нормальную структуру\n",
    "        metrics['avg_clustering'] = nx.average_clustering(G)\n",
    "        \n",
    "        # 3. Средняя степень узла - сколько в среднем связей у устройства\n",
    "        # Атакующие устройства часто имеют аномально высокую степень\n",
    "        degrees = [d for _, d in G.degree()]\n",
    "        metrics['avg_degree'] = sum(degrees) / len(degrees) if degrees else 0\n",
    "        \n",
    "        # 4. Энтропия центральности по степени - равномерность распределения связей\n",
    "        # Атаки часто вызывают аномалии в распределении связей\n",
    "        degree_cent = nx.degree_centrality(G)\n",
    "        values = list(degree_cent.values())\n",
    "        if values:\n",
    "            hist, _ = np.histogram(values, bins=10, density=True)\n",
    "            metrics['degree_centrality_entropy'] = -sum(p * np.log2(p) if p > 0 else 0 for p in hist)\n",
    "        else:\n",
    "            metrics['degree_centrality_entropy'] = 0\n",
    "        \n",
    "        # 5. Промежуточная центральность шлюза - насколько шлюз является мостом между другими узлами\n",
    "        # При атаке часто снижается, т.к. атакующий перенаправляет трафик через себя\n",
    "        if self.gateway_mac and self.gateway_mac in G:\n",
    "            betweenness = nx.betweenness_centrality(G)\n",
    "            metrics['gateway_betweenness'] = betweenness.get(self.gateway_mac, 0)\n",
    "        else:\n",
    "            metrics['gateway_betweenness'] = 0\n",
    "        \n",
    "        # 6. Модулярность сообществ - насколько чётко разделена сеть на группы устройств\n",
    "        # Атаки могут нарушать естественную структуру сообществ\n",
    "        try:\n",
    "            communities = nx.community.greedy_modularity_communities(G)\n",
    "            if communities:\n",
    "                metrics['community_modularity'] = nx.community.modularity(G, communities)\n",
    "            else:\n",
    "                metrics['community_modularity'] = 0\n",
    "        except:\n",
    "            metrics['community_modularity'] = 0\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Модель автоэнкодера с классификатором\n",
    "class ARPAttackDetector(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim=16, hidden_dims=[64, 32]):\n",
    "        super(ARPAttackDetector, self).__init__()\n",
    "        \n",
    "        # Энкодер\n",
    "        encoder_layers = []\n",
    "        prev_dim = input_dim\n",
    "        for dim in hidden_dims:\n",
    "            encoder_layers.append(nn.Linear(prev_dim, dim))\n",
    "            encoder_layers.append(nn.ReLU())\n",
    "            encoder_layers.append(nn.BatchNorm1d(dim))\n",
    "            prev_dim = dim\n",
    "        \n",
    "        encoder_layers.append(nn.Linear(prev_dim, latent_dim))\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Декодер\n",
    "        decoder_layers = []\n",
    "        prev_dim = latent_dim\n",
    "        for dim in reversed(hidden_dims):\n",
    "            decoder_layers.append(nn.Linear(prev_dim, dim))\n",
    "            decoder_layers.append(nn.ReLU())\n",
    "            decoder_layers.append(nn.BatchNorm1d(dim))\n",
    "            prev_dim = dim\n",
    "        \n",
    "        decoder_layers.append(nn.Linear(prev_dim, input_dim))\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "        \n",
    "        # Классификатор\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(32),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(32, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(16),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Кодирование входа\n",
    "        z = self.encoder(x)\n",
    "        \n",
    "        # Декодирование\n",
    "        x_recon = self.decoder(z)\n",
    "        \n",
    "        # Классификация\n",
    "        pred = self.classifier(z)\n",
    "        \n",
    "        return x_recon, pred\n",
    "    \n",
    "    def get_latent(self, x):\n",
    "        \"\"\"Получение латентного представления\"\"\"\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Функция обучения модели\n",
    "def train_model(X_train, y_train, input_dim, epochs=100, batch_size=64):\n",
    "    # Преобразуем данные в тензоры\n",
    "    X_tensor = torch.FloatTensor(X_train)\n",
    "    y_tensor = torch.FloatTensor(y_train).view(-1, 1)\n",
    "    \n",
    "    # Создаем датасет и загрузчик\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # Инициализируем модель\n",
    "    model = ARPAttackDetector(input_dim)\n",
    "    \n",
    "    # Функции потерь и оптимизатор\n",
    "    reconstruction_criterion = nn.MSELoss()\n",
    "    classification_criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    # Обучение\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        recon_loss_sum = 0\n",
    "        class_loss_sum = 0\n",
    "        \n",
    "        for X_batch, y_batch in dataloader:\n",
    "            # Прямой проход\n",
    "            X_recon, y_pred = model(X_batch)\n",
    "            \n",
    "            # Вычисление потерь\n",
    "            recon_loss = reconstruction_criterion(X_recon, X_batch)\n",
    "            class_loss = classification_criterion(y_pred, y_batch)\n",
    "            \n",
    "            # Общая потеря (с большим весом для классификации)\n",
    "            loss = 0.3 * recon_loss + 0.7 * class_loss\n",
    "            \n",
    "            # Обратное распространение\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            recon_loss_sum += recon_loss.item()\n",
    "            class_loss_sum += class_loss.item()\n",
    "        \n",
    "        # Вывод статистики каждые 10 эпох\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}, '\n",
    "                  f'Recon: {recon_loss_sum/len(dataloader):.4f}, '\n",
    "                  f'Class: {class_loss_sum/len(dataloader):.4f}')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Пример использования\n",
    "def main():\n",
    "    # Предположим, что у нас есть данные в формате DataFrame\n",
    "    # где каждая строка - признаки для ARP-пакета или окна пакетов\n",
    "     df = pd.read_csv(r'D:\\Проекты\\Дипломаня работа\\DoFitN\\Code\\DoFitN\\Data\\Data_20\\combined_features.csv')\n",
    "    \n",
    "    # Преобразование данных\n",
    "     X = df.drop('label', axis=1).values\n",
    "     y = df['label'].values\n",
    "    \n",
    "    # Масштабирование признаков\n",
    "     scaler = StandardScaler()\n",
    "     X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Разделение на обучающую и тестовую выборки\n",
    "     X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Обучение модели\n",
    "     model = train_model(X_train, y_train, input_dim=X_train.shape[1])\n",
    "    \n",
    "    # Оценка на тестовой выборке\n",
    "     X_test_tensor = torch.FloatTensor(X_test)\n",
    "     _, y_pred = model(X_test_tensor)\n",
    "     y_pred_np = y_pred.detach().numpy().flatten()\n",
    "     y_pred_binary = (y_pred_np > 0.5).astype(int)\n",
    "    \n",
    "    # Метрики качества\n",
    "     accuracy = (y_pred_binary == y_test).mean()\n",
    "     print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    \n",
    "     print(\"Модель создана и готова к обучению.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
